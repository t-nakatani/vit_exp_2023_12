{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44123e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q segmentation_models_pytorch==0.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58264475",
   "metadata": {},
   "source": [
    "***\n",
    "## 本実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4790953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__='2.1.2'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'{torch.__version__=}')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "exp_name = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c456281",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, d_model, nhead, num_encoder_layers, channels=3):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.n_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.embedding = nn.Conv2d(channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, self.n_patches, d_model))\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead), num_encoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch embeddings\n",
    "        x = self.embedding(x).flatten(2).transpose(1, 2)  # [B, n_patches, d_model]\n",
    "\n",
    "        # Add positional encodings\n",
    "        x += self.positional_encoding\n",
    "\n",
    "        # Pass through transformer\n",
    "        output = self.transformer(x)\n",
    "\n",
    "#         return output\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "#         print(d_model, nhead)\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_pad_mask=None):\n",
    "        \n",
    "        # Self attention with residual connection\n",
    "        attn_output1, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_pad_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(attn_output1))\n",
    "        \n",
    "        # Cross attention with residual connection\n",
    "#         print(f'')\n",
    "        attn_output2, attn_weight = self.multihead_attn(tgt, memory, memory)\n",
    "#         attn_output2, attn_weights = self.multihead_attn(tgt, memory, memory, key_padding_mask=tgt_pad_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(attn_output2))\n",
    "        \n",
    "        # Feed forward with residual connection\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout(ff_output)\n",
    "        \n",
    "#         print('before:', attn_weight.shape)\n",
    "        attn_weight = self.feed_forward(attn_weight)\n",
    "        attn_weight = self.feed_forward(attn_weight)\n",
    "#         print('after:', attn_weight.shape)\n",
    "        \n",
    "        return tgt, attn_weight\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, target_vocab_size, pad_id=-1):\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.decoder_layers = nn.ModuleList([CustomTransformerDecoderLayer(d_model, nhead) for _ in range(num_decoder_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def get_attn_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        tgt_pad_mask = (tgt == self.pad_id)\n",
    "        tgt = self.embedding(tgt) # tgt_after_embedding: [batch, target_len, d_model]\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # create masks, then pass to decoder\n",
    "        batch_size, target_len, d_model = tgt.shape\n",
    "        tgt_mask = self.get_attn_subsequent_mask(target_len)\n",
    "\n",
    "        tgt_mask = tgt_mask.to(tgt.device)\n",
    "\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "#             print(i, tgt_pad_mask.shape)\n",
    "            tgt, attn_weight = layer(tgt, memory, tgt_mask=tgt_mask, tgt_pad_mask=tgt_pad_mask)\n",
    "            output = self.fc_out(tgt)\n",
    "        \n",
    "        return output, attn_weight\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, d_model, nhead, num_encoder_layers, num_decoder_layers, target_vocab_size, channels=3):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = ViTEncoder(image_size, patch_size, d_model, nhead, num_encoder_layers, channels)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = CustomTransformerDecoder(d_model, nhead, num_decoder_layers, target_vocab_size)\n",
    "\n",
    "    def forward(self, img, tgt):\n",
    "        \"\"\"\n",
    "        tgt: [batch, target_len]\n",
    "        \"\"\"\n",
    "        # Encode image\n",
    "        encoder_output = self.encoder(img) # encoder_output: torch.Size([seq_len, batch, d_model])\n",
    "#         print(f'encoder_output: {encoder_output.shape}')\n",
    "        \n",
    "        # Decode to get the target sequence\n",
    "        output, cross_attn_maps = self.decoder(tgt, encoder_output)\n",
    "        \n",
    "        return output, cross_attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d330d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mask_proportions(mask, patch_size, flatten=True):\n",
    "    B, C, H, W = mask.shape\n",
    "    mask_proportions = torch.zeros(B, C, H // patch_size, W // patch_size)\n",
    "\n",
    "    for i in range(0, H, patch_size):\n",
    "        for j in range(0, W, patch_size):\n",
    "            patch = mask[:, :, i:i + patch_size, j:j + patch_size]\n",
    "            proportion = patch.sum(dim=[2, 3]) / (patch_size * patch_size)\n",
    "            mask_proportions[:, :, i // patch_size, j // patch_size] = proportion\n",
    "    if flatten:\n",
    "        B, C, H, W = mask_proportions.shape\n",
    "        reshaped_mask_proportions = mask_proportions.view(B, C, H * W)\n",
    "        return reshaped_mask_proportions\n",
    "\n",
    "    return mask_proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e10f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "class OneHotTensorEncode(object):\n",
    "    def __init__(self, n_classes=11):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __call__(self, mask, debug=False):\n",
    "        \"\"\"\n",
    "        Convert a 1-channel grayscale PIL image with n_classes unique values \n",
    "        into an n_classes-channel one-hot encoded tensor.\n",
    "        \"\"\"\n",
    "        mask = np.array(mask)\n",
    "        one_hot = torch.zeros(self.n_classes, mask.shape[0], mask.shape[1], dtype=torch.float32)\n",
    "        for i in range(self.n_classes):\n",
    "            one_hot[i, :, :] = torch.tensor(mask == i, dtype=torch.float32)\n",
    "            if debug:\n",
    "                count = np.sum(mask == i)\n",
    "                print(f\"Number of pixels with value {i}: {count}\")\n",
    "            \n",
    "                plt.imshow(one_hot[i, :, :].numpy(), cmap='gray')\n",
    "                plt.title(f\"Class {i} One-hot Encoded Channel\")\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "        return one_hot\n",
    "\n",
    "class RandomTransformsDual:\n",
    "    \"\"\"\n",
    "    画像とマスクの両方にランダム変換を適用するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        seed = np.random.randint(2147483647)  # 大きな数でランダムシードを設定\n",
    "        np.random.seed(seed)  # 同じシードを使用して、画像とマスクの両方に同じ変換を確実に適用する\n",
    "\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        for t in self.transforms:\n",
    "            mask = t(mask)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "def custom_crop(image, trim_margin=180):\n",
    "    return image.crop((trim_margin, trim_margin, image.width - trim_margin, image.height - trim_margin))\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_glob_path, image_transforms=None, mask_transforms=None, random_transforms=None):\n",
    "        self.image_glob_path = image_glob_path\n",
    "#         self.mask_glob_path = mask_glob_path\n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.random_transforms = random_transforms\n",
    "        self.image_paths = sorted(glob(image_glob_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = image_path.replace('/flw/', '/mask/')\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.random_transforms:\n",
    "            image, mask = self.random_transforms(image, mask)\n",
    "\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "\n",
    "        if self.mask_transforms:\n",
    "            mask = self.mask_transforms(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "random_transforms = RandomTransformsDual([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(180)  # ±180度のランダムな回転\n",
    "])\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Lambda(custom_crop),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.Lambda(custom_crop),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    OneHotTensorEncode(n_classes=11),\n",
    "])\n",
    "\n",
    "flw_glob_path = '../../../create_synthe_2023/synthetic_flw/flw/*/*'\n",
    "\n",
    "# image_glob_path, mask_glob_path, image_transforms=None, mask_transforms=None, random_transforms=None):\n",
    "dataset = SegmentationDataset(flw_glob_path, image_transforms=image_transforms, mask_transforms=mask_transforms)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.3\n",
    "\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "# test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c794af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 11, 256, 256])\n",
      "target_seq torch.Size([16, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 128, 32]' is invalid for input of size 1048576",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_seq\u001b[39m\u001b[38;5;124m'\u001b[39m, target_seq\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# target_seq = torch.arange(0, target_vocab_size).unsqueeze(1).repeat(1, batch_size)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# print('target_seq', target_seq.shape)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# # Sample input image and target sequence\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m output, cross_attention_maps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_seq.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_seq\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 137\u001b[0m, in \u001b[0;36mImageCaptioningModel.forward\u001b[0;34m(self, img, tgt)\u001b[0m\n\u001b[1;32m    133\u001b[0m         encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(img) \u001b[38;5;66;03m# encoder_output: torch.Size([seq_len, batch, d_model])\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#         print(f'encoder_output: {encoder_output.shape}')\u001b[39;00m\n\u001b[1;32m    135\u001b[0m         \n\u001b[1;32m    136\u001b[0m         \u001b[38;5;66;03m# Decode to get the target sequence\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m         output, cross_attn_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output, cross_attn_maps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mCustomTransformerDecoder.forward\u001b[0;34m(self, tgt, memory)\u001b[0m\n\u001b[1;32m    108\u001b[0m         tgt_mask \u001b[38;5;241m=\u001b[39m tgt_mask\u001b[38;5;241m.\u001b[39mto(tgt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers):\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#             print(i, tgt_pad_mask.shape)\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m             tgt, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_pad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_pad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m             output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(tgt)\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output, attn_weight\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mCustomTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, tgt_pad_mask)\u001b[0m\n\u001b[1;32m     67\u001b[0m         tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(tgt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output1))\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Cross attention with residual connection\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#         print(f'')\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m         attn_output2, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#         attn_output2, attn_weights = self.multihead_attn(tgt, memory, memory, key_padding_mask=tgt_pad_mask)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(tgt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output2))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5346\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5344\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5346\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[39;00m\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m static_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m bsz \u001b[38;5;241m*\u001b[39m num_heads, \\\n\u001b[1;32m   5350\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting static_k.size(0) of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatic_k\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 128, 32]' is invalid for input of size 1048576"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch.utils as smp_utils\n",
    "\n",
    "patch_size = 16\n",
    "\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "target_vocab_size = 11 # 出現しうる単語数(選択肢の数)\n",
    "len_out_seq = 11\n",
    "\n",
    "model = ImageCaptioningModel(image_size, patch_size, d_model, nhead, num_encoder_layers, num_decoder_layers, target_vocab_size)\n",
    "\n",
    "for i, (images, masks) in enumerate(train_dataloader):\n",
    "    print(masks.shape)\n",
    "    break\n",
    "# target_seq = torch.tensor(\n",
    "#     [j  if torch.sum(channel) != 0 else target_vocab_size - 1 for i, mask in enumerate(masks) for j, channel in enumerate(mask)]\n",
    "# ).reshape(masks.shape[:2]).transpose(0, 1)\n",
    "target_seq = torch.tensor(\n",
    "    [j  if torch.sum(channel) != 0 else target_vocab_size - 1 for i, mask in enumerate(masks) for j, channel in enumerate(mask)]\n",
    ").reshape(masks.shape[:2])\n",
    "print('target_seq', target_seq.shape)\n",
    "\n",
    "# target_seq = torch.arange(0, target_vocab_size).unsqueeze(1).repeat(1, batch_size)\n",
    "# print('target_seq', target_seq.shape)\n",
    "\n",
    "# # Sample input image and target sequence\n",
    "\n",
    "output, cross_attention_maps = model(images, target_seq)\n",
    "\n",
    "print(f'target_seq.shape: {target_seq.shape}')\n",
    "print(f'output.shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48e16b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = smp_utils.losses.DiceLoss().to(device)\n",
    "# criterion = smp_utils.losses.DiceLoss(ignore_channels=[0]).to(device)\n",
    "criterion_seq = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    sum_loss_mask = 0\n",
    "    sum_loss_seq = 0\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}, Train\") as pbar_train:\n",
    "        for i, (images, masks) in enumerate(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            target_seq = torch.tensor(\n",
    "                [j  if torch.sum(channel) != 0 else target_vocab_size - 1 for i, mask in enumerate(masks) for j, channel in enumerate(mask)]\n",
    "            ).reshape(masks.shape[:2]).to(device)\n",
    "            mask_proportions = calculate_mask_proportions(masks, patch_size).to(device)\n",
    "\n",
    "            output, cross_attention_map = model(images, target_seq)\n",
    "            \n",
    "            pred_probs = torch.sigmoid(output)\n",
    "            gt_one_hot = F.one_hot(target_seq, num_classes=11).float()\n",
    "            loss_seq = F.binary_cross_entropy(pred_probs, gt_one_hot) * 10\n",
    "            sum_loss_seq += loss_seq\n",
    "            \n",
    "#             loss_mask = criterion(cross_attention_map, mask_proportions)\n",
    "            loss_mask = criterion(cross_attention_map[:, 1:, :], mask_proportions[:, 1:, :])\n",
    "            sum_loss_mask += loss_mask\n",
    "            \n",
    "            loss = loss_mask  + loss_seq\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar_train.set_postfix({\"Train Loss\": loss.item()})\n",
    "            pbar_train.update(1)\n",
    "    print(f'loss_mask: {sum_loss_mask / (i + 1)}')\n",
    "    print(f'loss_seq: {sum_loss_seq / (i + 1)}')\n",
    "\n",
    "    model.eval()\n",
    "    sum_loss_mask = 0\n",
    "    sum_loss_seq = 0\n",
    "    total_val_accuracy = 0\n",
    "    with tqdm(total=len(val_dataloader), desc=f\"Epoch {epoch + 1} / {num_epochs}, Val\") as pbar_val:\n",
    "        for i, (images, masks) in enumerate(val_dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            target_seq = torch.tensor(\n",
    "                [j  if torch.sum(channel) != 0 else target_vocab_size - 1 for i, mask in enumerate(masks) for j, channel in enumerate(mask)]\n",
    "            ).reshape(masks.shape[:2]).to(device)\n",
    "            mask_proportions = calculate_mask_proportions(masks, patch_size).to(device)\n",
    "            \n",
    "            output, cross_attention_map = model(images, target_seq)\n",
    "            \n",
    "            pred_probs = torch.sigmoid(output)\n",
    "            gt_one_hot = F.one_hot(target_seq, num_classes=11).float()\n",
    "            loss_seq = F.binary_cross_entropy(pred_probs, gt_one_hot) * 10\n",
    "            sum_loss_seq += loss_seq\n",
    "            \n",
    "#             loss_mask = criterion(cross_attention_map, mask_proportions)\n",
    "            loss_mask = criterion(cross_attention_map[:, 1:, :], mask_proportions[:, 1:, :])\n",
    "            sum_loss_mask += loss_mask\n",
    "            \n",
    "#             loss = loss_mask\n",
    "            loss = loss_mask + loss_seq\n",
    "#             loss = loss_seq\n",
    "            \n",
    "            pbar_val.set_postfix({\"Val Loss\": loss.item()})\n",
    "            pbar_val.update(1)\n",
    "    print(f'loss_mask: {sum_loss_mask / (i + 1)}')\n",
    "    print(f'loss_seq: {sum_loss_seq / (i + 1)}')\n",
    "    \n",
    "    save_path = f'./models/{exp_name}_epoch{epoch + 1}'\n",
    "    torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2646b",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = ImageCaptioningModel(image_size, patch_size, d_model, nhead, num_encoder_layers, num_decoder_layers, target_vocab_size)\n",
    "# model_.load_state_dict(torch.load(save_path))\n",
    "# model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(the_model.state_dict(), PATH)\n",
    "\n",
    "# the_model = TheModelClass(*args, **kwargs)\n",
    "# the_model.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074632f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_attention_maps.to('cpu')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910899d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attention_maps.to('cpu')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4de4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attention_maps[:, 1:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ac4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in cross_attention_maps.to('cpu')[0]:\n",
    "    print(channel)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in mask_proportions.to('cpu')[0]:\n",
    "    print(channel)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea74fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd80b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9cf783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10]], device='cuda:2')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices = torch.argmax(pred_probs, dim=-1)\n",
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced4f060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9710, device='cuda:2', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d60fec84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2592, device='cuda:2', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de25b065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]], device='cuda:2')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices = torch.argmax(pred_probs, dim=-1)\n",
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7909af27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2662, device='cuda:2', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb1122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9672, device='cuda:2', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124809f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7469, device='cuda:2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934ffb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 11])\n",
      "tensor(0.7357)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(predicted_indices.shape)\n",
    "pred_one_hot = F.one_hot(predicted_indices).float()\n",
    "gt_one_hot = F.one_hot(target_seq).float()\n",
    "loss = F.binary_cross_entropy_with_logits(pred_one_hot, gt_one_hot)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bf5929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca29df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recreate_image_from_attention(cross_attention_map, img_size, patch_size):\n",
    "    \"\"\"\n",
    "    cross_attention_map: shape [11, 1024] (or any [channel_out, flattened_img_size])\n",
    "    img_size: Original image size (e.g., 224 for a 224x224 image)\n",
    "    patch_size: The size of each patch (e.g., 16 for 16x16 patches)\n",
    "    \"\"\"\n",
    "    channel_out, flattened_img_size = cross_attention_map.shape\n",
    "    patches_per_dim = img_size // patch_size\n",
    "    \n",
    "    # Reshape to [channel_out, patches_per_dim, patches_per_dim]\n",
    "    reshaped_attention_map = cross_attention_map.view(channel_out, patches_per_dim, patches_per_dim)\n",
    "    \n",
    "    # Initialize tensor to store the recreated image\n",
    "    recreated_images = torch.zeros((channel_out, img_size, img_size))\n",
    "    \n",
    "    # Fill in the recreated image tensor\n",
    "    for i in range(patches_per_dim):\n",
    "        for j in range(patches_per_dim):\n",
    "            recreated_images[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = reshaped_attention_map[:, i, j].unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "    return recreated_images\n",
    "\n",
    "\n",
    "# Call the function\n",
    "recreated_images = recreate_image_from_attention(cross_attention_maps[0][0], image_size, patch_size)\n",
    "\n",
    "# Visualize\n",
    "for i in range(recreated_images.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(recreated_images[i].detach().numpy(), cmap=\"gray\")\n",
    "    plt.title(f\"Channel {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreated_images = recreate_image_from_attention(mask_proportions[0], image_size, patch_size)\n",
    "\n",
    "# Visualize\n",
    "for i in range(recreated_images.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(recreated_images[i].detach().numpy(), cmap=\"gray\")\n",
    "    plt.title(f\"Channel {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97230f80",
   "metadata": {},
   "source": [
    "## Idea \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da6744",
   "metadata": {},
   "source": [
    "* クラスラベルの順列を出力系列とするのは、あまりに難しそう。\n",
    "* docoderに入力するのは、各パッチの特徴を並べたもの"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48146461",
   "metadata": {},
   "source": [
    "target_seqの系列長と最終的に出力してほしいsequence_lengthは一般には同じである必要はありません。Transformerモデルの訓練時においては、通常、ターゲット系列（target_seq）は入力として使われるだけでなく、訓練のラベルとしても使用されます。その場合、target_seqの系列長は出力と一致するように設計されることが多いです。\n",
    "\n",
    "しかし、推論（予測）時には、通常、開始トークンだけをtarget_seqとして使用し、モデルがその後のトークンを一つずつ生成するようにします。この場合、target_seqの初期の長さは1であり、出力のsequence_lengthは1から始まって任意の長さになり得ます（停止条件または最大長に達するまで）。\n",
    "\n",
    "簡単に言うと、訓練と推論でtarget_seqの役割が少し異なるため、その長さもそれに応じて変わります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32e7d030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices = torch.argmax(output, dim=-1)\n",
    "predicted_indices.shape == target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34417d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11564d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
