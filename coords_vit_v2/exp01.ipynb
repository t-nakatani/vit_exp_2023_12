{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577f55b0",
   "metadata": {},
   "source": [
    "## 変更点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fd60c",
   "metadata": {},
   "source": [
    "## 実験メモ\n",
    "\n",
    "#### 12/26\n",
    "* augumentationをするために、逐次マスクからcoordsに変換した方がいい\n",
    "* 角度は、0と2piでズレがほとんどないのにlossが大きくなってしまうので、単位円上の座標が良さそう\n",
    "* embeddingが無理？ embeddingは有限通りの入力をd_model次元で表現するものだから、従うためには有限通りの座標に落とし込む必要がある。となるとやはり角度が有利か。\n",
    "* 座標でembeddingするには、どのパッチの上に乗っているかをidxで表して、それを埋め込むのが良さそう\n",
    "\n",
    "#### 1/7 \n",
    "* positional Encodingのmax_len修正すべきかも"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b19ed",
   "metadata": {},
   "source": [
    ">推論時は、Greedyアルゴリズムの場合、再帰的に最初の単語から最後の単語まで一つづつ予測していきます（コード）。具体的には最初の推論でBOS（文章の始まり）のみを含む文章をターゲットとして、2個目の単語を予測します。そして予測した単語を含めて再度モデルを実行し、３個目の単語を予測します。このときモデルはソースであるドイツ語の文章と１〜２個目の英単語から３個目を予測することになります。これを繰り返し、モデルがEOS（文章の終わり）を出力するまで繰り返します。何番目の単語を予測するかで文章の長さが変わるのでtgt_maskは文章の長さを同じになるように値を設定します。\n",
    "\n",
    "https://qiita.com/simayan/items/ea8bc5df150f7890d0e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a383e796-dd8f-408b-9cd4-d40170bfa328",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'coordV2_exp1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faac4d0-3c92-4cc5-918f-ec1f4537f711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n",
      "2.1.2\n",
      "0.16.2\n",
      "0.3.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import segmentation_models_pytorch\n",
    "\n",
    "\n",
    "print(f'{torch.__version__}')\n",
    "print(f'{torchvision.__version__}')\n",
    "print(f'{segmentation_models_pytorch.__version__}')\n",
    "\n",
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58264475",
   "metadata": {},
   "source": [
    "***\n",
    "## 本実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3dc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "\n",
    "# # vit_b_16モデル呼び出し\n",
    "# model_vit = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "# # get_model()\n",
    "# model_vit = models.get_model('vit_b_16', weights='DEFAULT')\n",
    "\n",
    "# model_vit.heads = torch.nn.Identity()\n",
    "# torch.save(model_vit.state_dict(), 'vit_b_224_16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c456281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CustomVisionTransformer(models.VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomVisionTransformer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.heads = torch.nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        # print(f'_process_input: {x.shape=}')\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # 次の部分は一旦コメントあるとする、NLPにおけるtransformerと整合性があると思っているが、要確認\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        # x = x[:, 0]\n",
    "        # print(f'return: {x.shape=}')\n",
    "        return x\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(CustomTransformerDecoderLayer, self).__init__()\n",
    "#         print(d_model, nhead)\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_pad_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: shape []\n",
    "        \"\"\"\n",
    "        # Self attention with residual connection\n",
    "        attn_output1, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_pad_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(attn_output1))\n",
    "\n",
    "        # Cross attention with residual connection\n",
    "        attn_output2, attn_weights = self.multihead_attn(tgt, memory, memory)\n",
    "#         attn_output2, attn_weights = self.multihead_attn(tgt, memory, memory, key_padding_mask=tgt_pad_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(attn_output2))\n",
    "\n",
    "        # Feed forward with residual connection\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout(ff_output)\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, target_vocab_size, pad_id=0):\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.decoder_layers = nn.ModuleList([CustomTransformerDecoderLayer(d_model, nhead) for _ in range(num_decoder_layers)])\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def get_attn_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        tgt_pad_mask = (tgt == self.pad_id)\n",
    "        tgt = self.embedding(tgt) # tgt_after_embedding: [batch, target_len, d_model]\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # create masks, then pass to decoder\n",
    "        batch_size, target_len, d_model = tgt.shape\n",
    "        tgt_mask = self.get_attn_subsequent_mask(target_len)\n",
    "\n",
    "        tgt_mask = tgt_mask.to(tgt.device)\n",
    "\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, tgt_pad_mask=tgt_pad_mask)\n",
    "\n",
    "        # [batch, target_len?, d_model]\n",
    "        return tgt\n",
    "\n",
    "class CoordsHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoordsHead, self).__init__()\n",
    "\n",
    "class CoordsViT(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, d_model=768, nhead=12, num_encoder_layers=12, num_decoder_layers=12, target_vocab_size=11, channels=3, out_channel=2):\n",
    "        super(CoordsViT, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = CustomVisionTransformer(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_layers=num_encoder_layers,\n",
    "            num_heads=nhead,\n",
    "            hidden_dim=d_model,\n",
    "            mlp_dim=3072\n",
    "        )\n",
    "        self.encoder.load_state_dict(torch.load('vit_b_224_16.pth'))\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = CustomTransformerDecoder(d_model, nhead, num_decoder_layers, target_vocab_size)\n",
    "\n",
    "        # Coords Head\n",
    "        self.coords_heads = nn.Linear(d_model, out_channel)\n",
    "\n",
    "    def forward(self, img, tgt):\n",
    "        \"\"\"\n",
    "        tgt: [batch, target_len]\n",
    "        \"\"\"\n",
    "        # Encode image\n",
    "        encoder_output = self.encoder(img) # encoder_output: torch.Size([seq_len, batch, d_model])\n",
    "#         print(f'encoder_output: {encoder_output.shape}')\n",
    "\n",
    "        # Decode to get the target sequence\n",
    "        tgt_out = self.decoder(tgt, encoder_output)  # tgt_out: [batch, target_len?, d_model]\n",
    "\n",
    "        output = self.coords_heads(tgt_out)  # output: [batch, target_len?, 2], yx coords is 2d\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e10f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "class OneHotTensorEncode(object):\n",
    "    def __init__(self, n_classes=10):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __call__(self, mask, debug=False):\n",
    "        \"\"\"\n",
    "        Convert a 1-channel grayscale PIL image with n_classes unique values \n",
    "        into an n_classes-channel one-hot encoded tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        one_hot = torch.zeros(self.n_classes, mask.shape[0], mask.shape[1], dtype=torch.float32)\n",
    "        for i in range(self.n_classes):\n",
    "            one_hot[i, :, :] = torch.tensor(mask == i, dtype=torch.float32)\n",
    "            if debug:\n",
    "                count = np.sum(mask == i)\n",
    "                print(f\"Number of pixels with value {i}: {count}\")\n",
    "            \n",
    "                plt.imshow(one_hot[i, :, :].numpy(), cmap='gray')\n",
    "                plt.title(f\"Class {i} One-hot Encoded Channel\")\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "        return one_hot\n",
    "\n",
    "class RandomTransformsDual:\n",
    "    \"\"\"\n",
    "    画像とマスクの両方にランダム変換を適用するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        seed = np.random.randint(2147483647)  # 大きな数でランダムシードを設定\n",
    "        np.random.seed(seed)  # 同じシードを使用して、画像とマスクの両方に同じ変換を確実に適用する\n",
    "\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        for t in self.transforms:\n",
    "            mask = t(mask)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "def custom_crop(image, trim_margin=180):\n",
    "    return image.crop((trim_margin, trim_margin, image.width - trim_margin, image.height - trim_margin))\n",
    "\n",
    "def channelize(mask_array, n_classes=10):\n",
    "    mask_10d = torch.zeros(n_classes, mask_array.shape[0], mask_array.shape[1], dtype=torch.float32)\n",
    "    for i in range(n_classes): \n",
    "        mask_10d[i, :, :] = torch.tensor(mask == i, dtype=torch.float32)\n",
    "    return mask_10d\n",
    "\n",
    "def get_center_array(mask_10d, coord_filter, relative=False):\n",
    "    coords = []\n",
    "    for mask_i in mask_10d[1:]:\n",
    "        if int(mask_i.sum()) == 0:\n",
    "            coords.append([0, 0])\n",
    "            continue\n",
    "        center_yx = [int((mask_i * coord_filter[:, :, 0]).sum() / mask_i.sum()), int((mask_i * coord_filter[:, :, 1]).sum() / mask_i.sum())]\n",
    "        if relative:\n",
    "            center_yx[0] /= mask_i.shape[0]\n",
    "            center_yx[1] /= mask_i.shape[1]\n",
    "        coords.append(center_yx)\n",
    "    assert len(coords) == len(mask_10d) - 1, 'dim incorrect'\n",
    "    return torch.Tensor(coords)\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_glob_path, image_size, image_transforms=None, mask_transforms=None, random_transforms=None):\n",
    "        self.image_glob_path = image_glob_path\n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.random_transforms = random_transforms\n",
    "        self.image_paths = sorted(glob(image_glob_path)[:10000])\n",
    "        self.coord_filter = np.array([[(i, j) for j in range(image_size)] for i in range(image_size)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = image_path.replace('/flw/', '/mask/')\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.random_transforms:\n",
    "            image, mask = self.random_transforms(image, mask)\n",
    "\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "\n",
    "        if self.mask_transforms:\n",
    "            mask = self.mask_transforms(mask)\n",
    "            coords = get_center_array(mask, self.coord_filter)\n",
    "\n",
    "        return image, coords\n",
    "\n",
    "random_transforms = RandomTransformsDual([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(180)  # ±180度のランダムな回転\n",
    "])\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Lambda(custom_crop),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.Lambda(custom_crop),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    OneHotTensorEncode(n_classes=10),\n",
    "])\n",
    "\n",
    "flw_glob_path = '../../../../create_synthe_2023/synthetic_flw/flw/*/*'\n",
    "\n",
    "# image_glob_path, mask_glob_path, image_transforms=None, mask_transforms=None, random_transforms=None):\n",
    "dataset = SegmentationDataset(flw_glob_path, image_size, image_transforms=image_transforms, mask_transforms=mask_transforms)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.3\n",
    "\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "# test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "num_workers = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c794af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_seq.shape: torch.Size([16, 9])\n",
      "output.shape: torch.Size([16, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch.utils as smp_utils\n",
    "\n",
    "patch_size = 16\n",
    "num_patch_1d = (image_size // patch_size)\n",
    "indexer_1d = np.array([num_patch_1d, 1])\n",
    "\n",
    "d_model = 256\n",
    "# nhead = 8\n",
    "# num_encoder_layers = 3\n",
    "# num_decoder_layers = 3\n",
    "target_vocab_size = num_patch_1d ** 2 # patchの数を語彙数とする\n",
    "# len_out_seq = 9\n",
    "\n",
    "model = CoordsViT(target_vocab_size=target_vocab_size)\n",
    "\n",
    "for i, (images, coords) in enumerate(train_dataloader):\n",
    "    coords_idx_2d = coords // num_patch_1d\n",
    "    coords_idx_1d = coords_idx_2d @ indexer_1d\n",
    "    target_seq = coords_idx_1d.long()\n",
    "    break\n",
    "\n",
    "output = model(images, target_seq)\n",
    "\n",
    "print(f'target_seq.shape: {target_seq.shape}')\n",
    "print(f'output.shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d48e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100, Train: 100%|██████████| 438/438 [01:23<00:00,  5.25it/s, Train Loss=2.97e+3]\n",
      "Epoch 1 / 100, Val: 100%|██████████| 188/188 [00:27<00:00,  6.94it/s, Val Loss=2.88e+3]\n",
      "Epoch 2 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.21it/s, Train Loss=2.87e+3]\n",
      "Epoch 2 / 100, Val: 100%|██████████| 188/188 [00:23<00:00,  8.09it/s, Val Loss=2.95e+3]\n",
      "Epoch 3 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.20it/s, Train Loss=2.75e+3]\n",
      "Epoch 3 / 100, Val: 100%|██████████| 188/188 [00:20<00:00,  9.31it/s, Val Loss=2.67e+3]\n",
      "Epoch 4 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.16it/s, Train Loss=2.71e+3]\n",
      "Epoch 4 / 100, Val: 100%|██████████| 188/188 [00:25<00:00,  7.39it/s, Val Loss=2.66e+3]\n",
      "Epoch 5 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.19it/s, Train Loss=2.68e+3]\n",
      "Epoch 5 / 100, Val: 100%|██████████| 188/188 [00:18<00:00, 10.11it/s, Val Loss=2.65e+3]\n",
      "Epoch 6 / 100, Train: 100%|██████████| 438/438 [01:23<00:00,  5.22it/s, Train Loss=2.64e+3]\n",
      "Epoch 6 / 100, Val: 100%|██████████| 188/188 [00:32<00:00,  5.82it/s, Val Loss=2.6e+3] \n",
      "Epoch 7 / 100, Train: 100%|██████████| 438/438 [01:25<00:00,  5.10it/s, Train Loss=2.63e+3]\n",
      "Epoch 7 / 100, Val: 100%|██████████| 188/188 [00:37<00:00,  5.07it/s, Val Loss=2.6e+3] \n",
      "Epoch 8 / 100, Train: 100%|██████████| 438/438 [01:30<00:00,  4.83it/s, Train Loss=2.65e+3]\n",
      "Epoch 8 / 100, Val: 100%|██████████| 188/188 [00:32<00:00,  5.74it/s, Val Loss=2.77e+3]\n",
      "Epoch 9 / 100, Train: 100%|██████████| 438/438 [01:31<00:00,  4.80it/s, Train Loss=2.64e+3]\n",
      "Epoch 9 / 100, Val: 100%|██████████| 188/188 [00:41<00:00,  4.50it/s, Val Loss=2.71e+3]\n",
      "Epoch 10 / 100, Train: 100%|██████████| 438/438 [02:00<00:00,  3.63it/s, Train Loss=2.87e+3]\n",
      "Epoch 10 / 100, Val: 100%|██████████| 188/188 [00:58<00:00,  3.21it/s, Val Loss=2.99e+3]\n",
      "Epoch 11 / 100, Train: 100%|██████████| 438/438 [02:17<00:00,  3.19it/s, Train Loss=2.71e+3]\n",
      "Epoch 11 / 100, Val: 100%|██████████| 188/188 [00:59<00:00,  3.18it/s, Val Loss=3.25e+3]\n",
      "Epoch 12 / 100, Train: 100%|██████████| 438/438 [01:57<00:00,  3.73it/s, Train Loss=2.9e+3] \n",
      "Epoch 12 / 100, Val: 100%|██████████| 188/188 [00:46<00:00,  4.00it/s, Val Loss=2.97e+3]\n",
      "Epoch 13 / 100, Train: 100%|██████████| 438/438 [01:52<00:00,  3.88it/s, Train Loss=2.89e+3]\n",
      "Epoch 13 / 100, Val: 100%|██████████| 188/188 [01:04<00:00,  2.92it/s, Val Loss=2.91e+3]\n",
      "Epoch 14 / 100, Train: 100%|██████████| 438/438 [02:02<00:00,  3.57it/s, Train Loss=2.89e+3]\n",
      "Epoch 14 / 100, Val: 100%|██████████| 188/188 [00:51<00:00,  3.67it/s, Val Loss=2.94e+3]\n",
      "Epoch 15 / 100, Train: 100%|██████████| 438/438 [01:59<00:00,  3.66it/s, Train Loss=2.89e+3]\n",
      "Epoch 15 / 100, Val: 100%|██████████| 188/188 [00:50<00:00,  3.70it/s, Val Loss=2.93e+3]\n",
      "Epoch 16 / 100, Train: 100%|██████████| 438/438 [02:09<00:00,  3.39it/s, Train Loss=2.89e+3]\n",
      "Epoch 16 / 100, Val: 100%|██████████| 188/188 [00:52<00:00,  3.58it/s, Val Loss=2.91e+3]\n",
      "Epoch 17 / 100, Train: 100%|██████████| 438/438 [01:57<00:00,  3.72it/s, Train Loss=2.89e+3]\n",
      "Epoch 17 / 100, Val: 100%|██████████| 188/188 [00:55<00:00,  3.36it/s, Val Loss=2.9e+3] \n",
      "Epoch 18 / 100, Train: 100%|██████████| 438/438 [01:56<00:00,  3.77it/s, Train Loss=2.89e+3]\n",
      "Epoch 18 / 100, Val: 100%|██████████| 188/188 [00:54<00:00,  3.43it/s, Val Loss=3.04e+3]\n",
      "Epoch 19 / 100, Train: 100%|██████████| 438/438 [02:09<00:00,  3.37it/s, Train Loss=2.89e+3]\n",
      "Epoch 19 / 100, Val: 100%|██████████| 188/188 [00:58<00:00,  3.20it/s, Val Loss=2.88e+3]\n",
      "Epoch 20 / 100, Train: 100%|██████████| 438/438 [01:53<00:00,  3.87it/s, Train Loss=2.89e+3]\n",
      "Epoch 20 / 100, Val: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s, Val Loss=2.92e+3]\n",
      "Epoch 21 / 100, Train: 100%|██████████| 438/438 [02:01<00:00,  3.59it/s, Train Loss=2.89e+3]\n",
      "Epoch 21 / 100, Val: 100%|██████████| 188/188 [00:54<00:00,  3.44it/s, Val Loss=2.92e+3]\n",
      "Epoch 22 / 100, Train: 100%|██████████| 438/438 [01:57<00:00,  3.72it/s, Train Loss=2.89e+3]\n",
      "Epoch 22 / 100, Val: 100%|██████████| 188/188 [00:50<00:00,  3.75it/s, Val Loss=2.87e+3]\n",
      "Epoch 23 / 100, Train: 100%|██████████| 438/438 [02:03<00:00,  3.55it/s, Train Loss=2.88e+3]\n",
      "Epoch 23 / 100, Val: 100%|██████████| 188/188 [00:58<00:00,  3.22it/s, Val Loss=2.89e+3]\n",
      "Epoch 24 / 100, Train: 100%|██████████| 438/438 [02:00<00:00,  3.64it/s, Train Loss=2.88e+3]\n",
      "Epoch 24 / 100, Val: 100%|██████████| 188/188 [00:56<00:00,  3.33it/s, Val Loss=2.83e+3]\n",
      "Epoch 25 / 100, Train: 100%|██████████| 438/438 [02:03<00:00,  3.56it/s, Train Loss=2.79e+3]\n",
      "Epoch 25 / 100, Val: 100%|██████████| 188/188 [01:02<00:00,  3.02it/s, Val Loss=2.76e+3]\n",
      "Epoch 26 / 100, Train: 100%|██████████| 438/438 [02:14<00:00,  3.27it/s, Train Loss=2.72e+3]\n",
      "Epoch 26 / 100, Val: 100%|██████████| 188/188 [00:56<00:00,  3.34it/s, Val Loss=2.66e+3]\n",
      "Epoch 27 / 100, Train: 100%|██████████| 438/438 [02:20<00:00,  3.11it/s, Train Loss=2.67e+3]\n",
      "Epoch 27 / 100, Val: 100%|██████████| 188/188 [00:56<00:00,  3.32it/s, Val Loss=2.83e+3]\n",
      "Epoch 28 / 100, Train: 100%|██████████| 438/438 [02:18<00:00,  3.17it/s, Train Loss=2.64e+3]\n",
      "Epoch 28 / 100, Val: 100%|██████████| 188/188 [00:59<00:00,  3.17it/s, Val Loss=2.6e+3] \n",
      "Epoch 29 / 100, Train: 100%|██████████| 438/438 [02:16<00:00,  3.21it/s, Train Loss=2.64e+3]\n",
      "Epoch 29 / 100, Val: 100%|██████████| 188/188 [01:06<00:00,  2.85it/s, Val Loss=2.84e+3]\n",
      "Epoch 30 / 100, Train: 100%|██████████| 438/438 [02:22<00:00,  3.07it/s, Train Loss=2.62e+3]\n",
      "Epoch 30 / 100, Val: 100%|██████████| 188/188 [00:57<00:00,  3.27it/s, Val Loss=2.63e+3]\n",
      "Epoch 31 / 100, Train: 100%|██████████| 438/438 [02:18<00:00,  3.16it/s, Train Loss=2.61e+3]\n",
      "Epoch 31 / 100, Val: 100%|██████████| 188/188 [00:59<00:00,  3.13it/s, Val Loss=2.68e+3]\n",
      "Epoch 32 / 100, Train: 100%|██████████| 438/438 [02:33<00:00,  2.86it/s, Train Loss=2.61e+3]\n",
      "Epoch 32 / 100, Val: 100%|██████████| 188/188 [01:03<00:00,  2.95it/s, Val Loss=2.62e+3]\n",
      "Epoch 33 / 100, Train: 100%|██████████| 438/438 [02:17<00:00,  3.19it/s, Train Loss=2.64e+3]\n",
      "Epoch 33 / 100, Val: 100%|██████████| 188/188 [00:57<00:00,  3.24it/s, Val Loss=2.59e+3]\n",
      "Epoch 34 / 100, Train: 100%|██████████| 438/438 [02:29<00:00,  2.92it/s, Train Loss=2.64e+3]\n",
      "Epoch 34 / 100, Val: 100%|██████████| 188/188 [01:01<00:00,  3.04it/s, Val Loss=2.6e+3] \n",
      "Epoch 35 / 100, Train: 100%|██████████| 438/438 [02:16<00:00,  3.22it/s, Train Loss=2.59e+3]\n",
      "Epoch 35 / 100, Val: 100%|██████████| 188/188 [01:01<00:00,  3.04it/s, Val Loss=2.58e+3]\n",
      "Epoch 36 / 100, Train: 100%|██████████| 438/438 [02:24<00:00,  3.04it/s, Train Loss=2.62e+3]\n",
      "Epoch 36 / 100, Val: 100%|██████████| 188/188 [01:08<00:00,  2.75it/s, Val Loss=2.6e+3] \n",
      "Epoch 37 / 100, Train: 100%|██████████| 438/438 [02:14<00:00,  3.25it/s, Train Loss=2.59e+3]\n",
      "Epoch 37 / 100, Val: 100%|██████████| 188/188 [00:58<00:00,  3.20it/s, Val Loss=2.8e+3] \n",
      "Epoch 38 / 100, Train: 100%|██████████| 438/438 [02:27<00:00,  2.96it/s, Train Loss=2.82e+3]\n",
      "Epoch 38 / 100, Val: 100%|██████████| 188/188 [00:54<00:00,  3.44it/s, Val Loss=2.82e+3]\n",
      "Epoch 39 / 100, Train: 100%|██████████| 438/438 [02:49<00:00,  2.58it/s, Train Loss=2.78e+3]\n",
      "Epoch 39 / 100, Val: 100%|██████████| 188/188 [01:27<00:00,  2.15it/s, Val Loss=2.84e+3]\n",
      "Epoch 40 / 100, Train: 100%|██████████| 438/438 [03:29<00:00,  2.09it/s, Train Loss=2.75e+3]\n",
      "Epoch 40 / 100, Val: 100%|██████████| 188/188 [01:25<00:00,  2.21it/s, Val Loss=2.64e+3]\n",
      "Epoch 41 / 100, Train: 100%|██████████| 438/438 [03:32<00:00,  2.06it/s, Train Loss=2.64e+3]\n",
      "Epoch 41 / 100, Val: 100%|██████████| 188/188 [01:34<00:00,  1.99it/s, Val Loss=2.59e+3]\n",
      "Epoch 42 / 100, Train: 100%|██████████| 438/438 [03:00<00:00,  2.43it/s, Train Loss=2.65e+3]\n",
      "Epoch 42 / 100, Val: 100%|██████████| 188/188 [01:23<00:00,  2.26it/s, Val Loss=2.98e+3]\n",
      "Epoch 43 / 100, Train: 100%|██████████| 438/438 [02:39<00:00,  2.74it/s, Train Loss=2.73e+3]\n",
      "Epoch 43 / 100, Val: 100%|██████████| 188/188 [01:12<00:00,  2.59it/s, Val Loss=2.62e+3]\n",
      "Epoch 44 / 100, Train: 100%|██████████| 438/438 [02:44<00:00,  2.66it/s, Train Loss=2.63e+3]\n",
      "Epoch 44 / 100, Val: 100%|██████████| 188/188 [01:11<00:00,  2.63it/s, Val Loss=3.01e+3]\n",
      "Epoch 45 / 100, Train: 100%|██████████| 438/438 [02:38<00:00,  2.76it/s, Train Loss=2.69e+3]\n",
      "Epoch 45 / 100, Val: 100%|██████████| 188/188 [00:50<00:00,  3.74it/s, Val Loss=2.63e+3]\n",
      "Epoch 46 / 100, Train: 100%|██████████| 438/438 [02:18<00:00,  3.17it/s, Train Loss=2.6e+3] \n",
      "Epoch 46 / 100, Val: 100%|██████████| 188/188 [01:02<00:00,  3.00it/s, Val Loss=2.62e+3]\n",
      "Epoch 47 / 100, Train: 100%|██████████| 438/438 [01:43<00:00,  4.23it/s, Train Loss=2.81e+3]\n",
      "Epoch 47 / 100, Val: 100%|██████████| 188/188 [00:44<00:00,  4.22it/s, Val Loss=2.89e+3]\n",
      "Epoch 48 / 100, Train: 100%|██████████| 438/438 [01:32<00:00,  4.75it/s, Train Loss=2.89e+3]\n",
      "Epoch 48 / 100, Val: 100%|██████████| 188/188 [00:44<00:00,  4.24it/s, Val Loss=2.88e+3]\n",
      "Epoch 49 / 100, Train: 100%|██████████| 438/438 [01:43<00:00,  4.25it/s, Train Loss=2.88e+3]\n",
      "Epoch 49 / 100, Val: 100%|██████████| 188/188 [00:38<00:00,  4.85it/s, Val Loss=2.98e+3]\n",
      "Epoch 50 / 100, Train: 100%|██████████| 438/438 [01:30<00:00,  4.85it/s, Train Loss=2.86e+3]\n",
      "Epoch 50 / 100, Val: 100%|██████████| 188/188 [00:46<00:00,  4.02it/s, Val Loss=2.82e+3]\n",
      "Epoch 51 / 100, Train: 100%|██████████| 438/438 [01:33<00:00,  4.67it/s, Train Loss=2.82e+3]\n",
      "Epoch 51 / 100, Val: 100%|██████████| 188/188 [00:37<00:00,  5.00it/s, Val Loss=2.75e+3]\n",
      "Epoch 52 / 100, Train: 100%|██████████| 438/438 [01:33<00:00,  4.66it/s, Train Loss=2.7e+3] \n",
      "Epoch 52 / 100, Val: 100%|██████████| 188/188 [00:44<00:00,  4.27it/s, Val Loss=2.65e+3]\n",
      "Epoch 53 / 100, Train: 100%|██████████| 438/438 [01:29<00:00,  4.92it/s, Train Loss=2.66e+3]\n",
      "Epoch 53 / 100, Val: 100%|██████████| 188/188 [00:41<00:00,  4.53it/s, Val Loss=2.66e+3]\n",
      "Epoch 54 / 100, Train: 100%|██████████| 438/438 [01:39<00:00,  4.38it/s, Train Loss=2.63e+3]\n",
      "Epoch 54 / 100, Val: 100%|██████████| 188/188 [00:35<00:00,  5.32it/s, Val Loss=2.6e+3] \n",
      "Epoch 55 / 100, Train: 100%|██████████| 438/438 [01:30<00:00,  4.84it/s, Train Loss=2.62e+3]\n",
      "Epoch 55 / 100, Val: 100%|██████████| 188/188 [00:40<00:00,  4.68it/s, Val Loss=2.7e+3] \n",
      "Epoch 56 / 100, Train: 100%|██████████| 438/438 [01:33<00:00,  4.67it/s, Train Loss=2.61e+3]\n",
      "Epoch 56 / 100, Val: 100%|██████████| 188/188 [00:41<00:00,  4.52it/s, Val Loss=2.56e+3]\n",
      "Epoch 57 / 100, Train: 100%|██████████| 438/438 [01:36<00:00,  4.52it/s, Train Loss=2.59e+3]\n",
      "Epoch 57 / 100, Val: 100%|██████████| 188/188 [00:40<00:00,  4.67it/s, Val Loss=2.59e+3]\n",
      "Epoch 58 / 100, Train: 100%|██████████| 438/438 [01:29<00:00,  4.90it/s, Train Loss=2.64e+3]\n",
      "Epoch 58 / 100, Val: 100%|██████████| 188/188 [00:46<00:00,  4.07it/s, Val Loss=2.62e+3]\n",
      "Epoch 59 / 100, Train: 100%|██████████| 438/438 [01:29<00:00,  4.91it/s, Train Loss=2.59e+3]\n",
      "Epoch 59 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.67it/s, Val Loss=2.58e+3]\n",
      "Epoch 60 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.93it/s, Train Loss=2.57e+3]\n",
      "Epoch 60 / 100, Val: 100%|██████████| 188/188 [00:45<00:00,  4.11it/s, Val Loss=2.78e+3]\n",
      "Epoch 61 / 100, Train: 100%|██████████| 438/438 [01:30<00:00,  4.85it/s, Train Loss=2.56e+3]\n",
      "Epoch 61 / 100, Val: 100%|██████████| 188/188 [00:45<00:00,  4.13it/s, Val Loss=2.59e+3]\n",
      "Epoch 62 / 100, Train: 100%|██████████| 438/438 [01:35<00:00,  4.60it/s, Train Loss=2.57e+3]\n",
      "Epoch 62 / 100, Val: 100%|██████████| 188/188 [00:35<00:00,  5.29it/s, Val Loss=2.59e+3]\n",
      "Epoch 63 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.94it/s, Train Loss=2.56e+3]\n",
      "Epoch 63 / 100, Val: 100%|██████████| 188/188 [00:26<00:00,  7.01it/s, Val Loss=2.56e+3]\n",
      "Epoch 64 / 100, Train: 100%|██████████| 438/438 [01:35<00:00,  4.58it/s, Train Loss=2.61e+3]\n",
      "Epoch 64 / 100, Val: 100%|██████████| 188/188 [00:34<00:00,  5.40it/s, Val Loss=2.71e+3]\n",
      "Epoch 65 / 100, Train: 100%|██████████| 438/438 [01:33<00:00,  4.69it/s, Train Loss=2.56e+3]\n",
      "Epoch 65 / 100, Val: 100%|██████████| 188/188 [00:39<00:00,  4.76it/s, Val Loss=2.57e+3]\n",
      "Epoch 66 / 100, Train: 100%|██████████| 438/438 [01:30<00:00,  4.86it/s, Train Loss=2.54e+3]\n",
      "Epoch 66 / 100, Val: 100%|██████████| 188/188 [00:36<00:00,  5.11it/s, Val Loss=2.65e+3]\n",
      "Epoch 67 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.97it/s, Train Loss=2.53e+3]\n",
      "Epoch 67 / 100, Val: 100%|██████████| 188/188 [00:46<00:00,  4.02it/s, Val Loss=2.52e+3]\n",
      "Epoch 68 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.95it/s, Train Loss=2.58e+3]\n",
      "Epoch 68 / 100, Val: 100%|██████████| 188/188 [00:43<00:00,  4.33it/s, Val Loss=2.7e+3] \n",
      "Epoch 69 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.96it/s, Train Loss=2.55e+3]\n",
      "Epoch 69 / 100, Val: 100%|██████████| 188/188 [00:34<00:00,  5.39it/s, Val Loss=2.55e+3]\n",
      "Epoch 70 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.92it/s, Train Loss=2.53e+3]\n",
      "Epoch 70 / 100, Val: 100%|██████████| 188/188 [00:29<00:00,  6.31it/s, Val Loss=2.57e+3]\n",
      "Epoch 71 / 100, Train: 100%|██████████| 438/438 [01:36<00:00,  4.53it/s, Train Loss=2.54e+3]\n",
      "Epoch 71 / 100, Val: 100%|██████████| 188/188 [00:30<00:00,  6.18it/s, Val Loss=2.55e+3]\n",
      "Epoch 72 / 100, Train: 100%|██████████| 438/438 [01:32<00:00,  4.72it/s, Train Loss=2.53e+3]\n",
      "Epoch 72 / 100, Val: 100%|██████████| 188/188 [00:34<00:00,  5.40it/s, Val Loss=2.53e+3]\n",
      "Epoch 73 / 100, Train: 100%|██████████| 438/438 [01:29<00:00,  4.88it/s, Train Loss=2.52e+3]\n",
      "Epoch 73 / 100, Val: 100%|██████████| 188/188 [00:37<00:00,  5.07it/s, Val Loss=2.5e+3] \n",
      "Epoch 74 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.94it/s, Train Loss=2.5e+3] \n",
      "Epoch 74 / 100, Val: 100%|██████████| 188/188 [00:43<00:00,  4.28it/s, Val Loss=2.51e+3]\n",
      "Epoch 75 / 100, Train: 100%|██████████| 438/438 [01:32<00:00,  4.71it/s, Train Loss=2.5e+3] \n",
      "Epoch 75 / 100, Val: 100%|██████████| 188/188 [00:39<00:00,  4.80it/s, Val Loss=2.58e+3]\n",
      "Epoch 76 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.97it/s, Train Loss=2.5e+3] \n",
      "Epoch 76 / 100, Val: 100%|██████████| 188/188 [00:32<00:00,  5.71it/s, Val Loss=2.51e+3]\n",
      "Epoch 77 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.04it/s, Train Loss=2.5e+3] \n",
      "Epoch 77 / 100, Val: 100%|██████████| 188/188 [00:24<00:00,  7.61it/s, Val Loss=2.48e+3]\n",
      "Epoch 78 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.93it/s, Train Loss=2.54e+3]\n",
      "Epoch 78 / 100, Val: 100%|██████████| 188/188 [00:29<00:00,  6.46it/s, Val Loss=2.51e+3]\n",
      "Epoch 79 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.06it/s, Train Loss=2.51e+3]\n",
      "Epoch 79 / 100, Val: 100%|██████████| 188/188 [00:27<00:00,  6.96it/s, Val Loss=2.49e+3]\n",
      "Epoch 80 / 100, Train: 100%|██████████| 438/438 [01:27<00:00,  5.03it/s, Train Loss=2.54e+3]\n",
      "Epoch 80 / 100, Val: 100%|██████████| 188/188 [00:29<00:00,  6.27it/s, Val Loss=2.86e+3]\n",
      "Epoch 81 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.08it/s, Train Loss=2.89e+3]\n",
      "Epoch 81 / 100, Val: 100%|██████████| 188/188 [00:37<00:00,  5.05it/s, Val Loss=2.87e+3]\n",
      "Epoch 82 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.04it/s, Train Loss=2.89e+3]\n",
      "Epoch 82 / 100, Val: 100%|██████████| 188/188 [00:34<00:00,  5.50it/s, Val Loss=2.91e+3]\n",
      "Epoch 83 / 100, Train: 100%|██████████| 438/438 [01:27<00:00,  5.03it/s, Train Loss=2.89e+3]\n",
      "Epoch 83 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.61it/s, Val Loss=2.94e+3]\n",
      "Epoch 84 / 100, Train: 100%|██████████| 438/438 [01:25<00:00,  5.10it/s, Train Loss=2.88e+3]\n",
      "Epoch 84 / 100, Val: 100%|██████████| 188/188 [00:38<00:00,  4.93it/s, Val Loss=2.87e+3]\n",
      "Epoch 85 / 100, Train: 100%|██████████| 438/438 [01:27<00:00,  5.03it/s, Train Loss=2.88e+3]\n",
      "Epoch 85 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.56it/s, Val Loss=2.97e+3]\n",
      "Epoch 86 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.07it/s, Train Loss=2.88e+3]\n",
      "Epoch 86 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.68it/s, Val Loss=2.86e+3]\n",
      "Epoch 87 / 100, Train: 100%|██████████| 438/438 [01:28<00:00,  4.94it/s, Train Loss=2.88e+3]\n",
      "Epoch 87 / 100, Val: 100%|██████████| 188/188 [00:31<00:00,  5.96it/s, Val Loss=2.77e+3]\n",
      "Epoch 88 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.07it/s, Train Loss=2.77e+3]\n",
      "Epoch 88 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.68it/s, Val Loss=2.65e+3]\n",
      "Epoch 89 / 100, Train: 100%|██████████| 438/438 [01:36<00:00,  4.54it/s, Train Loss=2.79e+3]\n",
      "Epoch 89 / 100, Val: 100%|██████████| 188/188 [00:31<00:00,  6.02it/s, Val Loss=2.69e+3]\n",
      "Epoch 90 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.04it/s, Train Loss=2.59e+3]\n",
      "Epoch 90 / 100, Val: 100%|██████████| 188/188 [00:32<00:00,  5.82it/s, Val Loss=2.54e+3]\n",
      "Epoch 91 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.05it/s, Train Loss=2.59e+3]\n",
      "Epoch 91 / 100, Val: 100%|██████████| 188/188 [00:34<00:00,  5.49it/s, Val Loss=2.53e+3]\n",
      "Epoch 92 / 100, Train: 100%|██████████| 438/438 [01:25<00:00,  5.10it/s, Train Loss=2.53e+3]\n",
      "Epoch 92 / 100, Val: 100%|██████████| 188/188 [00:33<00:00,  5.53it/s, Val Loss=2.54e+3]\n",
      "Epoch 93 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.17it/s, Train Loss=2.5e+3] \n",
      "Epoch 93 / 100, Val: 100%|██████████| 188/188 [00:29<00:00,  6.33it/s, Val Loss=2.48e+3]\n",
      "Epoch 94 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.19it/s, Train Loss=2.49e+3]\n",
      "Epoch 94 / 100, Val: 100%|██████████| 188/188 [00:30<00:00,  6.23it/s, Val Loss=2.48e+3]\n",
      "Epoch 95 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.19it/s, Train Loss=2.49e+3]\n",
      "Epoch 95 / 100, Val: 100%|██████████| 188/188 [00:28<00:00,  6.66it/s, Val Loss=2.47e+3]\n",
      "Epoch 96 / 100, Train: 100%|██████████| 438/438 [01:25<00:00,  5.12it/s, Train Loss=2.49e+3]\n",
      "Epoch 96 / 100, Val: 100%|██████████| 188/188 [00:30<00:00,  6.26it/s, Val Loss=2.55e+3]\n",
      "Epoch 97 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.18it/s, Train Loss=2.49e+3]\n",
      "Epoch 97 / 100, Val: 100%|██████████| 188/188 [00:30<00:00,  6.15it/s, Val Loss=2.48e+3]\n",
      "Epoch 98 / 100, Train: 100%|██████████| 438/438 [01:24<00:00,  5.19it/s, Train Loss=2.48e+3]\n",
      "Epoch 98 / 100, Val: 100%|██████████| 188/188 [00:22<00:00,  8.36it/s, Val Loss=2.47e+3]\n",
      "Epoch 99 / 100, Train: 100%|██████████| 438/438 [01:23<00:00,  5.27it/s, Train Loss=2.5e+3] \n",
      "Epoch 99 / 100, Val: 100%|██████████| 188/188 [00:17<00:00, 10.59it/s, Val Loss=2.53e+3]\n",
      "Epoch 100 / 100, Train: 100%|██████████| 438/438 [01:26<00:00,  5.08it/s, Train Loss=2.48e+3]\n",
      "Epoch 100 / 100, Val: 100%|██████████| 188/188 [00:15<00:00, 12.20it/s, Val Loss=2.49e+3]\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    save_path = f'./models/{exp_name}_epoch{epoch + 1}.pth'\n",
    "    if os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        continue\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1} / {num_epochs}, Train\") as pbar_train:\n",
    "        for i, (images, coords) in enumerate(train_dataloader):\n",
    "            images = images.to(device)\n",
    "            coords_idx_2d = coords // num_patch_1d\n",
    "            coords_idx_1d = coords_idx_2d @ indexer_1d\n",
    "            target_seq = coords_idx_1d.long().to(device)\n",
    "\n",
    "            output = model(images, target_seq)\n",
    "            loss = criterion(output, coords.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()  # 各バッチのlossを追加\n",
    "            average_loss = total_loss / (i + 1)  # 平均lossを計算\n",
    "            pbar_train.set_postfix({\"Train Loss\": average_loss})  # 平均lossを表示\n",
    "            pbar_train.update(1)\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(val_dataloader), desc=f\"Epoch {epoch + 1} / {num_epochs}, Val\") as pbar_val:\n",
    "        for i, (images, coords) in enumerate(val_dataloader):\n",
    "            images = images.to(device)\n",
    "            coords_idx_2d = coords // num_patch_1d\n",
    "            coords_idx_1d = coords_idx_2d @ indexer_1d\n",
    "            target_seq = coords_idx_1d.long().to(device)\n",
    "\n",
    "            output = model(images, target_seq)\n",
    "            loss = criterion(output, coords.to(device))\n",
    "\n",
    "            total_loss += loss.item()  # 各バッチのlossを追加\n",
    "            average_loss = total_loss / (i + 1)  # 平均lossを計算\n",
    "            pbar_val.set_postfix({\"Val Loss\": average_loss})  # 平均lossを表示\n",
    "            pbar_val.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in cross_attention_maps.to('cpu')[0]:\n",
    "    print(channel)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020395a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in mask_proportions.to('cpu')[0]:\n",
    "    print(channel)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd80b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices = torch.argmax(pred_probs, dim=-1)\n",
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25b065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_indices = torch.argmax(pred_probs, dim=-1)\n",
    "predicted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124809f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7469, device='cuda:2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934ffb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 11])\n",
      "tensor(0.7357)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(predicted_indices.shape)\n",
    "pred_one_hot = F.one_hot(predicted_indices).float()\n",
    "gt_one_hot = F.one_hot(target_seq).float()\n",
    "loss = F.binary_cross_entropy_with_logits(pred_one_hot, gt_one_hot)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bf5929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5, 10, 10, 10, 10, 10],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7, 10, 10, 10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca29df2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def recreate_image_from_attention(cross_attention_map, img_size, patch_size):\n",
    "    \"\"\"\n",
    "    cross_attention_map: shape [11, 1024] (or any [channel_out, flattened_img_size])\n",
    "    img_size: Original image size (e.g., 224 for a 224x224 image)\n",
    "    patch_size: The size of each patch (e.g., 16 for 16x16 patches)\n",
    "    \"\"\"\n",
    "    channel_out, flattened_img_size = cross_attention_map.shape\n",
    "    patches_per_dim = img_size // patch_size\n",
    "    \n",
    "    # Reshape to [channel_out, patches_per_dim, patches_per_dim]\n",
    "    reshaped_attention_map = cross_attention_map.view(channel_out, patches_per_dim, patches_per_dim)\n",
    "    \n",
    "    # Initialize tensor to store the recreated image\n",
    "    recreated_images = torch.zeros((channel_out, img_size, img_size))\n",
    "    \n",
    "    # Fill in the recreated image tensor\n",
    "    for i in range(patches_per_dim):\n",
    "        for j in range(patches_per_dim):\n",
    "            recreated_images[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = reshaped_attention_map[:, i, j].unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "    return recreated_images\n",
    "\n",
    "\n",
    "# Call the function\n",
    "recreated_images = recreate_image_from_attention(cross_attention_maps[0][0], image_size, patch_size)\n",
    "\n",
    "# Visualize\n",
    "for i in range(recreated_images.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(recreated_images[i].detach().numpy(), cmap=\"gray\")\n",
    "    plt.title(f\"Channel {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreated_images = recreate_image_from_attention(mask_proportions[0], image_size, patch_size)\n",
    "\n",
    "# Visualize\n",
    "for i in range(recreated_images.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.imshow(recreated_images[i].detach().numpy(), cmap=\"gray\")\n",
    "    plt.title(f\"Channel {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97230f80",
   "metadata": {},
   "source": [
    "## Idea \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da6744",
   "metadata": {},
   "source": [
    "* クラスラベルの順列を出力系列とするのは、あまりに難しそう。\n",
    "* docoderに入力するのは、各パッチの特徴を並べたもの"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48146461",
   "metadata": {},
   "source": [
    "target_seqの系列長と最終的に出力してほしいsequence_lengthは一般には同じである必要はありません。Transformerモデルの訓練時においては、通常、ターゲット系列（target_seq）は入力として使われるだけでなく、訓練のラベルとしても使用されます。その場合、target_seqの系列長は出力と一致するように設計されることが多いです。\n",
    "\n",
    "しかし、推論（予測）時には、通常、開始トークンだけをtarget_seqとして使用し、モデルがその後のトークンを一つずつ生成するようにします。この場合、target_seqの初期の長さは1であり、出力のsequence_lengthは1から始まって任意の長さになり得ます（停止条件または最大長に達するまで）。\n",
    "\n",
    "簡単に言うと、訓練と推論でtarget_seqの役割が少し異なるため、その長さもそれに応じて変わります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32e7d030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indices = torch.argmax(output, dim=-1)\n",
    "predicted_indices.shape == target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34417d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11564d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
